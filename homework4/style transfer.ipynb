{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, transforms=None, max_size=None, shape=None):\n",
    "    image = Image.open(image_path)\n",
    "    image_size = image.size\n",
    "\n",
    "    if max_size is not None:\n",
    "        #获取图像size，为sequence\n",
    "        image_size = image.size\n",
    "        #转化为float的array\n",
    "        size = np.array(image_size).astype(float)\n",
    "        size = max_size / size * size\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "\n",
    "    if shape is not None:\n",
    "        image = image.resize(shape, Image.LANCZOS) #LANCZOS也是一种插值方法\n",
    "\n",
    "    #必须提供transform.ToTensor，转化为4D Tensor\n",
    "    if transforms is not None:\n",
    "        image = transforms(image).unsqueeze(0)\n",
    "\n",
    "    #是否拷贝到GPU\n",
    "    return image.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "#定义加载图像函数，并将PIL image转化为Tensor\n",
    "use_gpu = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_gpu else torch.FloatTensor\n",
    "print(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2019/04/10/5cadd5db873bd.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28']\n",
    "        self.vgg19 = models.vgg19(pretrained = True).features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        #name类型为str，x为Variable\n",
    "        for name, layer in self.vgg19._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 400, 400])\n",
      "torch.Size([1, 3, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "content = load_image('G:/warehouse/DL2019/hw4/content.jpg', transform, max_size = 400)\n",
    "style = load_image('G:/warehouse/DL2019/hw4/style.jpg', transform, shape = [content.size(2), content.size(3)])\n",
    "print(content.size())\n",
    "print(style.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = Variable(content.clone(), requires_grad = True)\n",
    "optimizer = torch.optim.Adam([target], lr = 0.002, betas=[0.5, 0.999])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = VGGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--lr'], dest='lr', nargs=None, const=None, default=0.003, type=<class 'float'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--content', type=str, default='./content.jpg')\n",
    "parser.add_argument('--style', type=str, default='./style.jpg')\n",
    "parser.add_argument('--max_size', type=int, default=400)\n",
    "parser.add_argument('--total_step', type=int, default=5000)\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=100)\n",
    "parser.add_argument('--style_weight', type=float, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/1000], Content Loss: 1.8715, Style Loss: 1073.7605\n",
      "Step [20/1000], Content Loss: 5.8217, Style Loss: 957.3817\n",
      "Step [30/1000], Content Loss: 9.4461, Style Loss: 868.0582\n",
      "Step [40/1000], Content Loss: 12.2200, Style Loss: 801.0866\n",
      "Step [50/1000], Content Loss: 14.3485, Style Loss: 748.8475\n",
      "Step [60/1000], Content Loss: 16.0742, Style Loss: 706.4912\n",
      "Step [70/1000], Content Loss: 17.4831, Style Loss: 671.1323\n",
      "Step [80/1000], Content Loss: 18.6602, Style Loss: 640.8476\n",
      "Step [90/1000], Content Loss: 19.6746, Style Loss: 614.3547\n",
      "Step [100/1000], Content Loss: 20.5530, Style Loss: 590.7620\n",
      "Step [110/1000], Content Loss: 21.3093, Style Loss: 569.4674\n",
      "Step [120/1000], Content Loss: 21.9826, Style Loss: 550.0508\n",
      "Step [130/1000], Content Loss: 22.5902, Style Loss: 532.1852\n",
      "Step [140/1000], Content Loss: 23.1442, Style Loss: 515.5713\n",
      "Step [150/1000], Content Loss: 23.6543, Style Loss: 500.0414\n",
      "Step [160/1000], Content Loss: 24.1185, Style Loss: 485.5206\n",
      "Step [170/1000], Content Loss: 24.5461, Style Loss: 471.8139\n",
      "Step [180/1000], Content Loss: 24.9418, Style Loss: 458.8423\n",
      "Step [190/1000], Content Loss: 25.3184, Style Loss: 446.5768\n",
      "Step [200/1000], Content Loss: 25.6651, Style Loss: 434.9188\n",
      "Step [210/1000], Content Loss: 25.9941, Style Loss: 423.8087\n",
      "Step [220/1000], Content Loss: 26.3067, Style Loss: 413.1869\n",
      "Step [230/1000], Content Loss: 26.6011, Style Loss: 403.0190\n",
      "Step [240/1000], Content Loss: 26.8772, Style Loss: 393.2752\n",
      "Step [250/1000], Content Loss: 27.1416, Style Loss: 383.9108\n",
      "Step [260/1000], Content Loss: 27.3926, Style Loss: 374.9023\n",
      "Step [270/1000], Content Loss: 27.6297, Style Loss: 366.2407\n",
      "Step [280/1000], Content Loss: 27.8580, Style Loss: 357.8942\n",
      "Step [290/1000], Content Loss: 28.0740, Style Loss: 349.8363\n",
      "Step [300/1000], Content Loss: 28.2852, Style Loss: 342.0359\n",
      "Step [310/1000], Content Loss: 28.4860, Style Loss: 334.4917\n",
      "Step [320/1000], Content Loss: 28.6738, Style Loss: 327.2186\n",
      "Step [330/1000], Content Loss: 28.8562, Style Loss: 320.1906\n",
      "Step [340/1000], Content Loss: 29.0259, Style Loss: 313.4099\n",
      "Step [350/1000], Content Loss: 29.1929, Style Loss: 306.8180\n",
      "Step [360/1000], Content Loss: 29.3546, Style Loss: 300.4428\n",
      "Step [370/1000], Content Loss: 29.5067, Style Loss: 294.2653\n",
      "Step [380/1000], Content Loss: 29.6533, Style Loss: 288.2860\n",
      "Step [390/1000], Content Loss: 29.7881, Style Loss: 282.5078\n",
      "Step [400/1000], Content Loss: 29.9220, Style Loss: 276.9123\n",
      "Step [410/1000], Content Loss: 30.0496, Style Loss: 271.4954\n",
      "Step [420/1000], Content Loss: 30.1749, Style Loss: 266.2301\n",
      "Step [430/1000], Content Loss: 30.2962, Style Loss: 261.1233\n",
      "Step [440/1000], Content Loss: 30.4127, Style Loss: 256.1685\n",
      "Step [450/1000], Content Loss: 30.5247, Style Loss: 251.3535\n",
      "Step [460/1000], Content Loss: 30.6368, Style Loss: 246.6883\n",
      "Step [470/1000], Content Loss: 30.7440, Style Loss: 242.1550\n",
      "Step [480/1000], Content Loss: 30.8521, Style Loss: 237.7502\n",
      "Step [490/1000], Content Loss: 30.9561, Style Loss: 233.4787\n",
      "Step [500/1000], Content Loss: 31.0580, Style Loss: 229.3230\n",
      "Step [510/1000], Content Loss: 31.1592, Style Loss: 225.2966\n",
      "Step [520/1000], Content Loss: 31.2569, Style Loss: 221.3790\n",
      "Step [530/1000], Content Loss: 31.3506, Style Loss: 217.5930\n",
      "Step [540/1000], Content Loss: 31.4433, Style Loss: 213.9209\n",
      "Step [550/1000], Content Loss: 31.5320, Style Loss: 210.3671\n",
      "Step [560/1000], Content Loss: 31.6181, Style Loss: 206.9192\n",
      "Step [570/1000], Content Loss: 31.7020, Style Loss: 203.5682\n",
      "Step [580/1000], Content Loss: 31.7810, Style Loss: 200.3315\n",
      "Step [590/1000], Content Loss: 31.8576, Style Loss: 197.1804\n",
      "Step [600/1000], Content Loss: 31.9319, Style Loss: 194.1145\n",
      "Step [610/1000], Content Loss: 32.0021, Style Loss: 191.1512\n",
      "Step [620/1000], Content Loss: 32.0712, Style Loss: 188.2747\n",
      "Step [630/1000], Content Loss: 32.1424, Style Loss: 185.4762\n",
      "Step [640/1000], Content Loss: 32.2081, Style Loss: 182.7630\n",
      "Step [650/1000], Content Loss: 32.2751, Style Loss: 180.1296\n",
      "Step [660/1000], Content Loss: 32.3368, Style Loss: 177.5733\n",
      "Step [670/1000], Content Loss: 32.3965, Style Loss: 175.0932\n",
      "Step [680/1000], Content Loss: 32.4534, Style Loss: 172.6826\n",
      "Step [690/1000], Content Loss: 32.5119, Style Loss: 170.3402\n",
      "Step [700/1000], Content Loss: 32.5665, Style Loss: 168.0683\n",
      "Step [710/1000], Content Loss: 32.6199, Style Loss: 165.8536\n",
      "Step [720/1000], Content Loss: 32.6744, Style Loss: 163.6958\n",
      "Step [730/1000], Content Loss: 32.7263, Style Loss: 161.6063\n",
      "Step [740/1000], Content Loss: 32.7759, Style Loss: 159.5674\n",
      "Step [750/1000], Content Loss: 32.8260, Style Loss: 157.5850\n",
      "Step [760/1000], Content Loss: 32.8756, Style Loss: 155.6537\n",
      "Step [770/1000], Content Loss: 32.9219, Style Loss: 153.7766\n",
      "Step [780/1000], Content Loss: 32.9702, Style Loss: 151.9455\n",
      "Step [790/1000], Content Loss: 33.0163, Style Loss: 150.1625\n",
      "Step [800/1000], Content Loss: 33.0623, Style Loss: 148.4256\n",
      "Step [810/1000], Content Loss: 33.1079, Style Loss: 146.7253\n",
      "Step [820/1000], Content Loss: 33.1523, Style Loss: 145.0742\n",
      "Step [830/1000], Content Loss: 33.1942, Style Loss: 143.4650\n",
      "Step [840/1000], Content Loss: 33.2369, Style Loss: 141.8894\n",
      "Step [850/1000], Content Loss: 33.2767, Style Loss: 140.3556\n",
      "Step [860/1000], Content Loss: 33.3178, Style Loss: 138.8548\n",
      "Step [870/1000], Content Loss: 33.3588, Style Loss: 137.3871\n",
      "Step [880/1000], Content Loss: 33.3937, Style Loss: 135.9506\n",
      "Step [890/1000], Content Loss: 33.4303, Style Loss: 134.5443\n",
      "Step [900/1000], Content Loss: 33.4644, Style Loss: 133.1756\n",
      "Step [910/1000], Content Loss: 33.4990, Style Loss: 131.8328\n",
      "Step [920/1000], Content Loss: 33.5339, Style Loss: 130.5158\n",
      "Step [930/1000], Content Loss: 33.5655, Style Loss: 129.2330\n",
      "Step [940/1000], Content Loss: 33.5988, Style Loss: 127.9766\n",
      "Step [950/1000], Content Loss: 33.6288, Style Loss: 126.7553\n",
      "Step [960/1000], Content Loss: 33.6612, Style Loss: 125.5533\n",
      "Step [970/1000], Content Loss: 33.6927, Style Loss: 124.3743\n",
      "Step [980/1000], Content Loss: 33.7237, Style Loss: 123.2209\n",
      "Step [990/1000], Content Loss: 33.7527, Style Loss: 122.0942\n",
      "Step [1000/1000], Content Loss: 33.7838, Style Loss: 120.9871\n"
     ]
    }
   ],
   "source": [
    "if use_gpu:\n",
    "    vgg = vgg.cuda()\n",
    "#config.log_step=10\n",
    "#config.sample_step=500\n",
    "for step in range(1000):  \n",
    "    target_features = vgg(target)\n",
    "    content_features = vgg(Variable(content))\n",
    "    style_features = vgg(Variable(style))\n",
    "    content_loss = 0.0\n",
    "    style_loss = 0.0\n",
    "    for f1,f2,f3 in zip(target_features, content_features, style_features):\n",
    "        content_loss += torch.mean((f1 - f2)**2)\n",
    "        n, c, h, w = f1.size()\n",
    "    #将特征reshape成二维矩阵相乘，求gram矩阵\n",
    "        f1 = f1.view(c, h * w)\n",
    "        f3 = f3.view(c, h * w)\n",
    "\n",
    "        f1 = torch.mm(f1, f1.t())\n",
    "        f3 = torch.mm(f3, f3.t())\n",
    "    \n",
    "    #计算style_loss\n",
    "        style_loss += torch.mean((f1 - f3)**2) / (c * h * w)\n",
    "        \n",
    "    loss = content_loss + style_loss \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (step+1) % 10 == 0:\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f'\n",
    "                   %(step+1, 1000, content_loss.data, style_loss.data))\n",
    "\n",
    "    if (step+1) % 1000 == 0:\n",
    "            # Save the generated image\n",
    "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "        img = target.clone().cpu().squeeze()\n",
    "        img = denorm(img.data).clamp_(0, 1)\n",
    "        torchvision.utils.save_image(img, 'output-%d.png' %(step+1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(np.array(content_features).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((f1-f2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
